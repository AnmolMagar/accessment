{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996b416",
   "metadata": {},
   "source": [
    "# Final Portfolio Project - Classification Task\n",
    "\n",
    "# Teen Phone Addiction Level Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06ea1f",
   "metadata": {},
   "source": [
    "## Task 1: Exploratory Data Analysis and Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53240b1c",
   "metadata": {},
   "source": [
    "### 1.1 Choosing a Dataset\n",
    "\n",
    "#### (a) When and by whom the dataset was created\n",
    "Provide the dataset provenance here (creator/owner and year, if known).\n",
    "\n",
    "#### (b) How and from where the dataset was accessed\n",
    "Provide the dataset source here (website / repository / provider) and the access date.\n",
    "\n",
    "#### (c) Alignment with United Nations Sustainable Development Goal (UNSDG)\n",
    "This dataset relates to **SDG 3: Good Health and Well-Being** because excessive smartphone use can impact sleep quality, mental health, and overall well-being.\n",
    "\n",
    "#### (d) List and brief description of all attributes (features)\n",
    "Use the column list table (next code cell) to write a brief description of each feature in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing / Model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# (works both on local folder and the provided environment)\n",
    "try:\n",
    "    df = pd.read_csv(\"teen_phone_addiction_dataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv(\"/mnt/data/teen_phone_addiction_dataset.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97135ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column list (use this to write brief descriptions in the markdown section above)\n",
    "pd.DataFrame({\"column\": df.columns, \"dtype\": [str(df[c].dtype) for c in df.columns]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286ff7c",
   "metadata": {},
   "source": [
    "#### Potential Questions the Dataset Can Answer\n",
    "\n",
    "1. Which behavioral factors (daily usage, checks/day, screen time before bed) are most associated with higher addiction?\n",
    "2. Does higher phone usage correlate with lower sleep hours or lower academic performance?\n",
    "3. Are there demographic differences (age, gender, grade) in predicted addiction category?\n",
    "\n",
    "#### Dataset Quality Assessment\n",
    "\n",
    "The next cells check missing values, duplicates, and basic distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and duplicates\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"Missing values (top):\")\n",
    "display(missing[missing>0].head(20))\n",
    "\n",
    "print(\"\\nDuplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eeeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary statistics for numeric columns\n",
    "df.describe(include=[np.number]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect categorical columns\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "pd.DataFrame({\"categorical_column\": cat_cols, \"n_unique\": [df[c].nunique() for c in cat_cols]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f830d",
   "metadata": {},
   "source": [
    "### 1.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "#### (a) Data Cleaning and Preprocessing\n",
    "\n",
    "For classification, the dataset provides a continuous **Addiction_Level** score. We convert it into a categorical target using quantile-based binning (Low / Medium / High) to create a meaningful and reasonably balanced classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification target (3 balanced classes) from Addiction_Level\n",
    "df = df.copy()\n",
    "\n",
    "# Ensure Addiction_Level is numeric\n",
    "df[\"Addiction_Level\"] = pd.to_numeric(df[\"Addiction_Level\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where target is missing\n",
    "df = df.dropna(subset=[\"Addiction_Level\"]).reset_index(drop=True)\n",
    "\n",
    "# Quantile binning into 3 classes\n",
    "df[\"Addiction_Class\"] = pd.qcut(df[\"Addiction_Level\"], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "df[[\"Addiction_Level\", \"Addiction_Class\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance\n",
    "class_counts = df[\"Addiction_Class\"].value_counts().sort_index()\n",
    "class_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Class Distribution: Addiction_Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed864e39",
   "metadata": {},
   "source": [
    "#### (b) Visualizations to Summarize, Explore, and Understand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features (excluding ID)\n",
    "num_cols = [c for c in df.columns if df[c].dtype != \"object\" and c not in [\"ID\"]]\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between phone usage and sleep\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(data=df, x=\"Daily_Usage_Hours\", y=\"Sleep_Hours\", hue=\"Addiction_Class\", alpha=0.7)\n",
    "plt.title(\"Daily Phone Usage vs Sleep Hours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic performance across addiction classes\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(data=df, x=\"Addiction_Class\", y=\"Academic_Performance\")\n",
    "plt.title(\"Academic Performance by Addiction Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phone checks per day across addiction classes\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.boxplot(data=df, x=\"Addiction_Class\", y=\"Phone_Checks_Per_Day\")\n",
    "plt.title(\"Phone Checks per Day by Addiction Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical distribution example: Gender vs Addiction class\n",
    "plt.figure(figsize=(6,4))\n",
    "pd.crosstab(df[\"Gender\"], df[\"Addiction_Class\"]).plot(kind=\"bar\")\n",
    "plt.title(\"Gender vs Addiction Class\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c04863",
   "metadata": {},
   "source": [
    "# Task 2: Build a Neural Network Model\n",
    "\n",
    "We use an MLPClassifier with a preprocessing pipeline (imputation + one-hot encoding for categorical features + scaling for numeric features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a521e",
   "metadata": {},
   "source": [
    "# Task 3: Build Primary Model\n",
    "\n",
    "## 3.1 Split Dataset into Training and Testing Sets\n",
    "\n",
    "The same train/test split from Task 2 is used.\n",
    "\n",
    "## 3.2 Model A: Logistic Regression\n",
    "## 3.3 Model B: Random Forest Classifier\n",
    "## 3.4 Initial Comparison and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network classifier\n",
    "mlp_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = mlp_pipe.predict(X_train)\n",
    "y_pred_test = mlp_pipe.predict(X_test)\n",
    "\n",
    "def cls_metrics(y_true, y_pred, label=\"\"):\n",
    "    return pd.Series({\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision (weighted)\": precision_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"Recall (weighted)\": recall_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"F1 (weighted)\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "    }, name=label)\n",
    "\n",
    "metrics_train = cls_metrics(y_train, y_pred_train, \"Train\")\n",
    "metrics_test = cls_metrics(y_test, y_pred_test, \"Test\")\n",
    "\n",
    "pd.concat([metrics_train, metrics_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbb0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (test)\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[\"Low\",\"Medium\",\"High\"])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low\",\"Medium\",\"High\"])\n",
    "disp.plot()\n",
    "plt.title(\"MLPClassifier - Confusion Matrix (Test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541199",
   "metadata": {},
   "source": [
    "# Task 3: Build Primary Model\n",
    "n## 3.1 Split Dataset into Training and Testing Sets\n",
    "\n",
    "The same train/test split from Task 2 is used.\n",
    "\n",
    "## 3.2 Model A: Logistic Regression\n",
    "## 3.3 Model B: Random Forest Classifier\n",
    "## 3.4 Initial Comparison and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dead06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Logistic Regression (multinomial)\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, multi_class=\"auto\")\n",
    "\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", log_reg)\n",
    "])\n",
    "\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "y_pred_lr = logreg_pipe.predict(X_test)\n",
    "\n",
    "lr_metrics = cls_metrics(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "lr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49194ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: Random Forest\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", rf_clf)\n",
    "])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipe.predict(X_test)\n",
    "\n",
    "rf_metrics = cls_metrics(y_test, y_pred_rf, \"Random Forest\")\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial comparison table (test set)\n",
    "initial_comparison = pd.DataFrame([lr_metrics, rf_metrics]).reset_index().rename(columns={\"index\":\"Model\"})\n",
    "initial_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e538b",
   "metadata": {},
   "source": [
    "# Task 4: Hyper-parameter Optimization with Cross-Validation\n",
    "\n",
    "We tune the two classical ML models with GridSearchCV using a pipeline. The scoring metric is weighted F1 to handle any class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93677d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "f1_weighted = make_scorer(f1_score, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# 4.1 Logistic Regression tuning\n",
    "logreg_param_grid = {\n",
    "    \"model__C\": [0.1, 1, 5, 10],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "logreg_gs = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid=logreg_param_grid,\n",
    "    scoring=f1_weighted,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg_gs.fit(X_train, y_train)\n",
    "\n",
    "logreg_gs.best_params_, logreg_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Random Forest tuning\n",
    "rf_param_grid = {\n",
    "    \"model__n_estimators\": [200, 400],\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "rf_gs = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=f1_weighted,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_gs.fit(X_train, y_train)\n",
    "\n",
    "rf_gs.best_params_, rf_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Summary of best hyperparameters and CV scores\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"Logistic Regression\", \"Best CV Score (F1w)\": logreg_gs.best_score_, \"Best Params\": logreg_gs.best_params_},\n",
    "    {\"Model\": \"Random Forest\", \"Best CV Score (F1w)\": rf_gs.best_score_, \"Best Params\": rf_gs.best_params_},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686913b",
   "metadata": {},
   "source": [
    "# Task 5: Feature Selection\n",
    "\n",
    "A **filter method** is applied using mutual information. After preprocessing, SelectKBest keeps the top-k most informative features. This is applied for both classical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose k based on feature space size (after one-hot).\n",
    "# We start with a moderate k and can adjust if desired.\n",
    "k_best = 20\n",
    "\n",
    "fs_logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=mutual_info_classif, k=k_best)),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "fs_rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=mutual_info_classif, k=k_best)),\n",
    "    (\"model\", RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=300, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Cross-validated score (using weighted F1)\n",
    "fs_logreg_cv = cross_val_score(fs_logreg_pipe, X_train, y_train, cv=5, scoring=f1_weighted, n_jobs=-1).mean()\n",
    "fs_rf_cv = cross_val_score(fs_rf_pipe, X_train, y_train, cv=5, scoring=f1_weighted, n_jobs=-1).mean()\n",
    "\n",
    "fs_logreg_cv, fs_rf_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb719f",
   "metadata": {},
   "source": [
    "# Task 6: Final Models and Comparative Analysis\n",
    "\n",
    "Rebuild both classical models using:\n",
    "- Best hyperparameters from Task 4\n",
    "- Feature selection from Task 5\n",
    "\n",
    "Then evaluate on the test set and compare in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Logistic Regression with best hyperparameters + feature selection\n",
    "final_logreg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=mutual_info_classif, k=k_best)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        C=logreg_gs.best_params_[\"model__C\"],\n",
    "        penalty=logreg_gs.best_params_[\"model__penalty\"],\n",
    "        solver=logreg_gs.best_params_[\"model__solver\"]\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Final Random Forest with best hyperparameters + feature selection\n",
    "bp = rf_gs.best_params_\n",
    "final_rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=mutual_info_classif, k=k_best)),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=bp[\"model__n_estimators\"],\n",
    "        max_depth=bp[\"model__max_depth\"],\n",
    "        min_samples_split=bp[\"model__min_samples_split\"],\n",
    "        min_samples_leaf=bp[\"model__min_samples_leaf\"],\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit + evaluate\n",
    "final_logreg.fit(X_train, y_train)\n",
    "final_rf.fit(X_train, y_train)\n",
    "\n",
    "pred_logreg = final_logreg.predict(X_test)\n",
    "pred_rf = final_rf.predict(X_test)\n",
    "\n",
    "final_lr_metrics = cls_metrics(y_test, pred_logreg, \"Final Logistic Regression\")\n",
    "final_rf_metrics = cls_metrics(y_test, pred_rf, \"Final Random Forest\")\n",
    "\n",
    "final_lr_metrics, final_rf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table (similar to Table 1 / Table 4 in the assignment)\n",
    "comparison_table = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Logistic Regression (Final)\",\n",
    "        \"Features\": f\"SelectKBest(k={k_best})\",\n",
    "        \"CV Score (F1w)\": logreg_gs.best_score_,\n",
    "        \"Accuracy\": final_lr_metrics[\"Accuracy\"],\n",
    "        \"Precision\": final_lr_metrics[\"Precision (weighted)\"],\n",
    "        \"Recall\": final_lr_metrics[\"Recall (weighted)\"],\n",
    "        \"F1-Score\": final_lr_metrics[\"F1 (weighted)\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Random Forest (Final)\",\n",
    "        \"Features\": f\"SelectKBest(k={k_best})\",\n",
    "        \"CV Score (F1w)\": rf_gs.best_score_,\n",
    "        \"Accuracy\": final_rf_metrics[\"Accuracy\"],\n",
    "        \"Precision\": final_rf_metrics[\"Precision (weighted)\"],\n",
    "        \"Recall\": final_rf_metrics[\"Recall (weighted)\"],\n",
    "        \"F1-Score\": final_rf_metrics[\"F1 (weighted)\"],\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e8f41",
   "metadata": {},
   "source": [
    "## Task 7: Report Quality and Presentation\n",
    "\n",
    "- Code is organized into tasks and uses pipelines for reproducibility.\n",
    "- Visualizations are labeled with titles and axes.\n",
    "- Tables summarize key results.\n",
    "\n",
    "## Task 8: Conclusion and Reflection\n",
    "\n",
    "1. **Model Performance:** State which final model performed best and cite the key metrics.\n",
    "2. **Impact of Methods:** Explain how hyperparameter tuning and feature selection affected results.\n",
    "3. **Insights and Future Directions:** Summarize insights from EDA and suggest future improvements (e.g., different class thresholds, more feature engineering, other models)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
